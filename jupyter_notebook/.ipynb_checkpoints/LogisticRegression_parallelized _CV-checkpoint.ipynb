{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Part0 Config\n",
    "#global variables\n",
    "averages = []\n",
    "col_sum = []\n",
    "stds = []\n",
    "N_COLS = 57\n",
    "N_ROWS = 0\n",
    "\n",
    "fileName ='../data/spam.data'\n",
    "\n",
    "#########################################\n",
    "#Part1 readFile()\n",
    "#########################################   \n",
    "\n",
    "#read each line and split it to 57 features and 1 label    \n",
    "def inputMapper(line):\n",
    "    cols = line.split()\n",
    "    cols = [float(col) for col in cols]\n",
    "    return(cols[0:57],cols[57])\n",
    "    \n",
    "def readFile(fileName):\n",
    "    global N_ROWS\n",
    "    rdd = sc.textFile(fileName)\n",
    "    N_ROWS = rdd.count()\n",
    "    return rdd.map(inputMapper).cache()\n",
    "    \n",
    "#########################################\n",
    "#Part2 standardize\n",
    "#########################################\n",
    "def calculate_averages(n_cols,n_rows,col_sum):\n",
    "    mean = []\n",
    "    i = 0\n",
    "    while(i < n_cols):\n",
    "        mean.append (col_sum[i]/n_rows)\n",
    "        i += 1\n",
    "    return mean\n",
    "\n",
    "\n",
    "def sumReducer(x,y):\n",
    "    xf = x[0]\n",
    "    yf = y[0]\n",
    "    i = 0\n",
    "    out = []\n",
    "    while i < N_COLS:\n",
    "        out.append(xf[i] + yf[i])\n",
    "        i += 1\n",
    "    return (out,0)\n",
    "\n",
    "def residuesMapper(x):\n",
    "    xf = x[0]\n",
    "    xl = x[1]\n",
    "    i = 0\n",
    "    out =[]\n",
    "    while i < N_COLS:\n",
    "        out.append( np.math.pow(xf[i] - averages[i],2))\n",
    "        i += 1\n",
    "    return (out,xl)\n",
    "\n",
    "def calculate_stds(n_cols,n_rows,sigmas):\n",
    "    i = 0\n",
    "    out =[]\n",
    "    while i < n_cols:\n",
    "        out.append(np.math.sqrt(sigmas[i]/n_rows))\n",
    "        i += 1\n",
    "    return out\n",
    "    \n",
    "def standardizeMapper(x):\n",
    "    features = x[0]\n",
    "    label = x[1]\n",
    "    out0 = []\n",
    "    i=0\n",
    "    while i < N_COLS:\n",
    "        f = features[i]\n",
    "        t = (f - averages[i])/stds[i]\n",
    "        out0.append(t)\n",
    "        i += 1\n",
    "    return (out0,label)\n",
    "  \n",
    "\n",
    "\n",
    "def standardize (RDD_Xy):\n",
    "    global col_sum\n",
    "    global averages\n",
    "    global stds\n",
    "    col_sum = RDD_Xy.reduce(sumReducer)[0]\n",
    "    averages =  calculate_averages(N_COLS,N_ROWS,col_sum)\n",
    "    sigmas = RDD_Xy.map(residuesMapper).reduce(sumReducer)[0]\n",
    "    stds = calculate_stds(N_COLS,N_ROWS,sigmas)\n",
    "    rdd = RDD_Xy.map(standardizeMapper)\n",
    "    rdd.cache().count()\n",
    "    return rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "#Part3 train\n",
    "#########################################\n",
    "#global variables for training\n",
    "w = np.ones(N_COLS)\n",
    "b = 0\n",
    "dw = np.zeros(N_COLS)\n",
    "db = 0\n",
    "cost_list = []\n",
    "\n",
    "def mlog(x):\n",
    "    if(x == 0):\n",
    "        return -100000\n",
    "    return np.math.log(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    #deal with overflow\n",
    "    if(x>700):\n",
    "        return 1\n",
    "    if(x<-700):\n",
    "        return 0\n",
    "    res = 1/(1 + np.math.exp(-x))\n",
    "    return res\n",
    "    \n",
    "def calculate_t(x,w,b):\n",
    "    t = 0\n",
    "    i = 0\n",
    "    while i < N_COLS :\n",
    "        t += x[i]*w[i]\n",
    "        i += 1\n",
    "    t = t + b\n",
    "    return t\n",
    "\n",
    "\n",
    "def y_estimated_mapper(x):\n",
    "    global w\n",
    "    global b\n",
    "    features = x[0]\n",
    "    labels = x[1]\n",
    "    z = sigmoid(calculate_t(features,w,b))\n",
    "    return(features,z,labels)\n",
    "\n",
    "def loss_mapper(RDD_Xzy):\n",
    "    x = RDD_Xzy[0]\n",
    "    z = RDD_Xzy[1]\n",
    "    y = RDD_Xzy[2]\n",
    "    return y*mlog(z)+(1-y)*mlog(1-z)\n",
    "\n",
    "def loss_reducer(x,y):\n",
    "     return x + y\n",
    "\n",
    "def calculate_model_complexity(W,n_rows,lambda_reg):\n",
    "    complexity = 0\n",
    "    for w in W :\n",
    "        complexity += w*w\n",
    "    complexity = complexity * lambda_reg /n_rows\n",
    "    return complexity\n",
    "\n",
    "\n",
    "def calculate_cost(RDD_Xzy,lambda_reg):\n",
    "    global w\n",
    "    loss = RDD_Xzy.map(loss_mapper).reduce(loss_reducer)\n",
    "    model_complexity = calculate_model_complexity(w,N_ROWS,lambda_reg)\n",
    "    cost = -loss/N_ROWS + model_complexity\n",
    "    return cost\n",
    "\n",
    "def gradient_mapper(RDD_Xzy):\n",
    "    x = RDD_Xzy[0]\n",
    "    z = RDD_Xzy[1]\n",
    "    y = RDD_Xzy[2]\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < N_COLS:\n",
    "        t = x[i]*(z-y)\n",
    "        out.append(t)\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "def gradient_reducer(x,y):\n",
    "    i = 0\n",
    "    out = []\n",
    "    while i < N_COLS:\n",
    "        t= x[i] + y[i]\n",
    "        out.append(t)\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "def calculate_gradients(RDD_Xzy,lambda_reg):\n",
    "    global w\n",
    "    out = []\n",
    "    temp = RDD_Xzy.map(gradient_mapper).reduce(gradient_reducer)\n",
    "    i = 0\n",
    "    while i < N_COLS:\n",
    "        out.append(temp[i] + lambda_reg * w[i]/ N_ROWS)\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def calculate_db(RDD_Xzy):\n",
    "    return RDD_Xzy.map(lambda x : x[1] - x [2]).reduce(lambda x,y: x+y)\n",
    "\n",
    "def update_weights(w,b,dw,db,learning_rate):\n",
    "    i = 0\n",
    "    while i < N_COLS:\n",
    "        w[i] = w[i] - learning_rate * dw[i]\n",
    "        i += 1\n",
    "    b = b - learning_rate * db\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train (RDD_Xy, iterations, learning_rate, lambda_reg):\n",
    "    #initialize weights and bias\n",
    "    dw=0\n",
    "    db=0\n",
    "    global w\n",
    "    global b\n",
    "    global N_ROWS\n",
    "    N_ROWS = RDD_Xy.count()\n",
    "    w = np.ones(N_COLS)\n",
    "    b = 0\n",
    "    for iteration in range(iterations):\n",
    "        print('iteration',iteration)\n",
    "        #propagation\n",
    "        RDD_Xzy = RDD_Xy.map(y_estimated_mapper)\n",
    "        #calculate Cost function\n",
    "        cost = calculate_cost(RDD_Xzy,lambda_reg)\n",
    "        cost_list.append(cost)\n",
    "        #calculate accuracy\n",
    "        #backPropagation\n",
    "        dw = calculate_gradients(RDD_Xzy,lambda_reg)\n",
    "        db = calculate_db(RDD_Xzy)\n",
    "        #Update weights and bias with Gradient descent\n",
    "        w,b = update_weights(w,b,dw,db,learning_rate)\n",
    "    return w,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_test(x):\n",
    "    if x >0.8:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def predictMapper(RDD_Xy):\n",
    "    x,z,y = y_estimated_mapper(RDD_Xy)\n",
    "    z = predict_test(z)\n",
    "    return (z,y)\n",
    "\n",
    "def predict(w_, b_, RDD_Xy):\n",
    "    global w \n",
    "    global b\n",
    "    w = w_\n",
    "    b = b_\n",
    "    return RDD_Xy.map(predictMapper)\n",
    "\n",
    "def accuracyMapper(RDD_Zy):\n",
    "\n",
    "    z = RDD_Zy[0]\n",
    "    y = RDD_Zy[1]\n",
    "    return np.math.fabs(z - y)\n",
    "\n",
    "def accuracy(w_, b_, RDD_Xy):\n",
    "    global w \n",
    "    global b\n",
    "    w = w_\n",
    "    b = b_\n",
    "    n_rows = RDD_Xy.count()\n",
    "    RDD_Zy = predict(w_,b_,RDD_Xy)\n",
    "    sum_error = RDD_Zy.map(accuracyMapper).reduce(lambda x ,y : x + y)\n",
    "    acc_avg = 1 - sum_error/n_rows\n",
    "    return acc_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_transfrom(RDD_Xy,k_fold):\n",
    "    weights = np.ones(10)\n",
    "    weights = weights.tolist()\n",
    "    RDD_folds = RDD_Xy.randomSplit(weights,6)\n",
    "    return RDD_folds\n",
    "\n",
    "\n",
    "def get_block_data(RDD_folds,i):\n",
    "    RDD_test = RDD_folds[i]\n",
    "    RDD_train = None\n",
    "    for k in range(len(RDD_folds)):\n",
    "        if k != i:\n",
    "            if RDD_train == None:\n",
    "                RDD_train =  RDD_folds[k]\n",
    "            else:\n",
    "                RDD_train = RDD_train.union(RDD_folds[k])\n",
    "    RDD_train.count()\n",
    "    return RDD_test,RDD_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_avgerage_accuracy(acus):\n",
    "    aveg = 0\n",
    "    for acu in acus:\n",
    "        aveg += acu\n",
    "    return aveg/len(acus)\n",
    "\n",
    "def cross_validation(k_fold = 10,iterations=10, learning_rate=0.7, lambda_reg=0.4):\n",
    "    RDD_Xy = readFile('../data/spam.data')\n",
    "    RDD_SXy = standardize(RDD_Xy)\n",
    "    RDD_folds = shuffle_transfrom(RDD_SXy,k_fold)\n",
    "    acus = []\n",
    "    for i in range(k_fold):\n",
    "        print('cv',i)\n",
    "        RDD_test,RDD_train = get_block_data(RDD_folds,i)\n",
    "        print(RDD_test.count(),RDD_train.count())\n",
    "        w,d = train(RDD_train,iterations,learning_rate,lambda_reg)\n",
    "        acu = accuracy(w,b,RDD_test)\n",
    "        acus.append(acu)\n",
    "        print('accuracy',acu)\n",
    "    avg_acu = calculate_avgerage_accuracy(acus)\n",
    "    print(avg_acu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_validation(10,30,0.7,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('test',cost_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.4.8\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
